{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1417c28a-dc5a-4a20-840a-5001faeceab9",
   "metadata": {},
   "source": [
    "# Propdesk Data Pipeline - Seasonality and IO-bound processes\n",
    "### Example: Calculate seasonality for a pair in a given exchange, check if everything is ok and set up a periodic job to keep it updated\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ef5f39-40fd-4fe6-ad62-52e24fb3be12",
   "metadata": {},
   "source": [
    "### Most of examples in notebook 1 are useful and/or the same to compute/retrieve seasonality. Check it for reference. \n",
    "#### This notebook highlights some details and differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6b93e94-ae4e-4a3b-922b-02875cbd8d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "['adabrl']\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from propdesk_tardis.tardis_transfero import tardis_transfero as tardis\n",
    "\n",
    "exchange = 'binance'\n",
    "items_to_search = ['ada', 'brl']\n",
    "\n",
    "all_datasets = tardis.get_all_exchange_datasets(exchange)\n",
    "print([i for i in all_datasets.keys() if all([s in i for s in items_to_search])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ddca4285-d49e-46b9-9f1e-6f90e22df91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pair = 'btcusdt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa05b759-c945-4985-a390-a4a8108a1929",
   "metadata": {},
   "source": [
    "### Let's say we want to calculate it from 2021-11-20 to 2021-12-15. \n",
    "## Note that dates are **[date_from, date_to)**, so this will effectively calculate data until ***2021-12-14 23:59:59***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d85e73e-7dce-4d16-b8cd-e60fa9bcf3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_from = '2021-11-20'\n",
    "date_to = '2021-12-15'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f02822c-1a76-423e-9bc9-3ab1d19e5ce7",
   "metadata": {},
   "source": [
    "Then, we can define the parameters of the seasonality estimation process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7bbe4c9b-cf18-467e-b4ab-e0c5bcff16e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_dict = {\n",
    "    'exchange': exchange,\n",
    "    'pair': pair,\n",
    "    'start_date': date_from,\n",
    "    'end_date': date_to,\n",
    "    'lookback_days':30,\n",
    "    'resampling_rule': '60S'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46c22cd-526c-4d48-aed3-fd32a3e0ff80",
   "metadata": {},
   "source": [
    "We can simply run this job to have it computed. As an example, let's see if the script is deployed to databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b63763aa-0fda-49af-abd0-0e72068d5fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['spark_schedule_handler.py', 'spark_seasonality.py', 'spark_volatility_zma.py']\n"
     ]
    }
   ],
   "source": [
    "from propdesk_azure_services.azure_databricks import list_databricks_src_files\n",
    "deployed_files = list_databricks_src_files()\n",
    "print(deployed_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "886db262-ef1c-43f1-903c-3e6166106db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEREEE\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'run_id': 220169,\n",
       " 'job_id': 8138,\n",
       " 'run_page_url': 'https://adb-3928083337264192.12.azuredatabricks.net/?o=3928083337264192#job/8138/run/1'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_name = f'{pair}_{exchange}_seasonality'\n",
    "script_to_run = 'spark_seasonality.py'\n",
    "job_type = 'io_intensive'\n",
    "\n",
    "from propdesk_azure_services.azure_databricks import single_run_job\n",
    "\n",
    "# function commented because it would trigger a run and return the data structure printed below\n",
    "# single_run_job(job_name, script_to_run, params_dict, job_type=job_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfc74da-d029-4b47-858c-56d5b8cb5b5e",
   "metadata": {},
   "source": [
    "# ⚠️ ATTENTION - job_type = 'io_intensive'\n",
    "\n",
    "Seasonality is an **I/O-BOUND** job. This means that donwloading data dominates the time to complete the job. We download a large amount of data (namely, `lookback_days=30`) and work lightly on that large amount of data (we downsample dates to 60S ticks).\n",
    "That means it does make some sense to parallelize the calculations (using Spark), but not the distribution of tasks in the Spark nodes. This would cause the creation of a task run for a particular day ***N***, and another for a particular day ***N+1***.\n",
    "Task for day ***N*** will download data from day ***N-30*** and task for day ***N-1*** will overlap downloading data from ***N+1-30***. This causes an overload of requests on a provider, e.g., Tardis and is completely redundant overloading the cluster.\n",
    "\n",
    "Therefore, we can schedule a single task job (which will download all data, once, with no overlap) by limiting `max_tasks_in_job=1`. This is done automatically when passing `job_type='io_intensive'`. Also, `task_len = 31` limits each task to calculate up to 31 days.\n",
    "\n",
    "Just pass the correct job type and everything will be fine :)\n",
    "# ⚠️ -----------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa7db3b-b064-49a1-8545-1b66fb3a633e",
   "metadata": {},
   "source": [
    "### Connecting to storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f473395-0872-44a8-94d8-1c5c2f1733ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from propdesk_estimators.exchange_storage import ExchangeStorage\n",
    "binance = ExchangeStorage(exchange) # -- ExchangeStorage('binance')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b972c8a-6b36-4c05-b406-ab69b59091dc",
   "metadata": {},
   "source": [
    "Check for datasets that were already computed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "98aeebc5-2f95-4724-ad8b-13ce18e18408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset_type': 'seasonality', 'pair': 'adabrl', 'date_from': '2021-11-20', 'date_to': '2021-12-15', 'lookback_days': 30, 'resampling_rule': '60s'}\n"
     ]
    }
   ],
   "source": [
    "query_dict = {\n",
    "    'dataset_type': 'seasonality',\n",
    "    'pair': pair,\n",
    "    'date_from': date_from,\n",
    "    'date_to': date_to,\n",
    "    'lookback_days': 30,\n",
    "    'resampling_rule': '60s'\n",
    "}\n",
    "print(query_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "449574ac-d802-4c26-9a8c-9dfb35c48396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already computed datasets in query\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['adabrl/seasonality/2021/11/adabrl_seasonality_2021-11-20',\n",
       " 'adabrl/seasonality/2021/11/adabrl_seasonality_2021-11-21',\n",
       " 'adabrl/seasonality/2021/11/adabrl_seasonality_2021-11-22',\n",
       " 'adabrl/seasonality/2021/11/adabrl_seasonality_2021-11-23',\n",
       " 'adabrl/seasonality/2021/11/adabrl_seasonality_2021-11-24',\n",
       " 'adabrl/seasonality/2021/11/adabrl_seasonality_2021-11-25',\n",
       " 'adabrl/seasonality/2021/11/adabrl_seasonality_2021-11-26',\n",
       " 'adabrl/seasonality/2021/11/adabrl_seasonality_2021-11-27',\n",
       " 'adabrl/seasonality/2021/11/adabrl_seasonality_2021-11-28',\n",
       " 'adabrl/seasonality/2021/11/adabrl_seasonality_2021-11-29',\n",
       " 'adabrl/seasonality/2021/11/adabrl_seasonality_2021-11-30',\n",
       " 'adabrl/seasonality/2021/12/adabrl_seasonality_2021-12-01',\n",
       " 'adabrl/seasonality/2021/12/adabrl_seasonality_2021-12-02',\n",
       " 'adabrl/seasonality/2021/12/adabrl_seasonality_2021-12-03',\n",
       " 'adabrl/seasonality/2021/12/adabrl_seasonality_2021-12-04',\n",
       " 'adabrl/seasonality/2021/12/adabrl_seasonality_2021-12-05',\n",
       " 'adabrl/seasonality/2021/12/adabrl_seasonality_2021-12-06',\n",
       " 'adabrl/seasonality/2021/12/adabrl_seasonality_2021-12-07',\n",
       " 'adabrl/seasonality/2021/12/adabrl_seasonality_2021-12-08',\n",
       " 'adabrl/seasonality/2021/12/adabrl_seasonality_2021-12-09',\n",
       " 'adabrl/seasonality/2021/12/adabrl_seasonality_2021-12-10',\n",
       " 'adabrl/seasonality/2021/12/adabrl_seasonality_2021-12-11',\n",
       " 'adabrl/seasonality/2021/12/adabrl_seasonality_2021-12-12',\n",
       " 'adabrl/seasonality/2021/12/adabrl_seasonality_2021-12-13',\n",
       " 'adabrl/seasonality/2021/12/adabrl_seasonality_2021-12-14']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print ('Already computed datasets in query')\n",
    "binance.list_datasets_by_params(query_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196710b4-5f6b-4c7b-a6db-34185e1f4c56",
   "metadata": {},
   "source": [
    "## We could have used the method amend to find only the days that we are missing and generate a dict to process those days\n",
    "(check notebook 1 on volatility)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b403a52-733e-48d4-b551-72d7ed46ebba",
   "metadata": {},
   "source": [
    "### With the module `propdesk_estimators.exchange_storage`, we can interact with those datasets. Let's straight up get a dataframe with the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "61cf330b-745b-431f-b63e-b22d3a06fec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files saved to: /tmp/tmp48wt0d10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>seasonality_estimation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-11-20 00:00:00</td>\n",
       "      <td>283.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-11-20 00:01:00</td>\n",
       "      <td>679.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-11-20 00:02:00</td>\n",
       "      <td>451.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-11-20 00:03:00</td>\n",
       "      <td>500.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-11-20 00:04:00</td>\n",
       "      <td>351.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35995</th>\n",
       "      <td>2021-12-14 23:55:00</td>\n",
       "      <td>187.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35996</th>\n",
       "      <td>2021-12-14 23:56:00</td>\n",
       "      <td>345.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35997</th>\n",
       "      <td>2021-12-14 23:57:00</td>\n",
       "      <td>130.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35998</th>\n",
       "      <td>2021-12-14 23:58:00</td>\n",
       "      <td>433.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35999</th>\n",
       "      <td>2021-12-14 23:59:00</td>\n",
       "      <td>356.30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>36000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 datetime  seasonality_estimation\n",
       "0     2021-11-20 00:00:00                  283.65\n",
       "1     2021-11-20 00:01:00                  679.25\n",
       "2     2021-11-20 00:02:00                  451.20\n",
       "3     2021-11-20 00:03:00                  500.50\n",
       "4     2021-11-20 00:04:00                  351.95\n",
       "...                   ...                     ...\n",
       "35995 2021-12-14 23:55:00                  187.55\n",
       "35996 2021-12-14 23:56:00                  345.20\n",
       "35997 2021-12-14 23:57:00                  130.15\n",
       "35998 2021-12-14 23:58:00                  433.60\n",
       "35999 2021-12-14 23:59:00                  356.30\n",
       "\n",
       "[36000 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from propdesk_estimators.exchange_storage import get_dataframe_by_params\n",
    "\n",
    "# pass the flag keep_local to keep raw files instead of downloading them again if needed; keep_local=False will delete the files\n",
    "adabrl_seas_df = get_dataframe_by_params(exchange_str='binance', params_dict=query_dict, keep_local=True)\n",
    "adabrl_seas_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba56306f-bb7b-42c2-9d7e-50797ff19e19",
   "metadata": {},
   "source": [
    "# Success :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db37207e-8fd0-4b12-b9d1-e9a6da9d1849",
   "metadata": {},
   "source": [
    "## Creating a daily job\n",
    "Now, after using, checking data, etc., we can create a periodic (weekly/daily) job to keep this dataset updated. We'll setup it to run everyday at 04:00 AM Sao Paulo time, to use the cluster at an idle time. \n",
    "\n",
    "## Ideally, **check Databricks UI to see if there are jobs scheduled for that time and day to avoid overloading the cluster**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b93357cd-36e9-44fc-a5e1-b1e132e49fcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'job': {'job_id': 10089}, 'schedule_job': {'job_id': 10169}}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from propdesk_azure_services.azure_databricks import create_periodic_job\n",
    "\n",
    "exchange = 'binance'\n",
    "# daily job\n",
    "job_name = f'daily_seasonality_{pair}_{exchange}'\n",
    "script_to_run = 'spark_seasonality.py'\n",
    "\n",
    "##############\n",
    "# make sure to have a schedule that is compatible with the period\n",
    "# you can validate it in https://www.freeformatter.com/cron-expression-generator-quartz.html\n",
    "# this one reads 'everyday at 4PM'\n",
    "# defautl timezone is AMERICAS-SAO PAULO\n",
    "job_schedule = \"0 0 4 * * ?\"\n",
    "#############\n",
    "\n",
    "period = 'daily'\n",
    "# no need for start_date and end_date in daily jobs \n",
    "job_params = {\n",
    "    'exchange': exchange,\n",
    "    'pair': pair,\n",
    "    'lookback_days':30,\n",
    "    'resampling_rule': '60S'\n",
    "}\n",
    "\n",
    "\n",
    "create_periodic_job(job_name_str=job_name,\n",
    "                    filename_str=script_to_run,\n",
    "                    params_dict=job_params,\n",
    "                    period=period,\n",
    "                    cron_expression_str=job_schedule)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff1bfdd-6017-4b64-a2ef-5aff899b2c75",
   "metadata": {},
   "source": [
    "## That's it. **check Databricks UI to make sure everything is ok**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbf7bca-9ce3-4c1c-90a4-e52908785ba7",
   "metadata": {},
   "source": [
    "### Have fun, move fast, break things, buy btc (or dcr or algorand) ⚡.\n",
    "#### -- Propdesk Transfero"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transferoenv",
   "language": "python",
   "name": "transferoenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
